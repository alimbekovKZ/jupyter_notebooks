# coding: utf-8
import warnings
import gc
import pickle
import time
import kaggle
import os


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold

from sklearn import metrics


from multiprocessing import Manager, Process, Queue, current_process


print("script started: ", time.strftime("%b %d %Y %H:%M:%S"))

experinment_nr = 4

train_fname = 'train_target_mean_full.pkl'
test_fname = 'test_target_mean_full.pkl'

if train_fname.endswith('.gz'):
    TRAIN = pd.read_pickle(train_fname, compression='gzip')
else:    
    TRAIN = pd.read_pickle(train_fname)

print("TRAIN LOADED")
#train_mean = pd.read_pickle('../input/train_target_mean_no_versions.pkl')
#TRAIN = pd.concat([TRAIN, train_mean], axis=1)
#del train_mean
gc.collect()
print("TRAIN PREPARED")


TARGET = pd.read_pickle('target.pkl')

if test_fname.endswith('.gz'):
    TEST = pd.read_pickle(test_fname, compression='gzip')
else:
    TEST = pd.read_pickle(test_fname)
#test_mean = pd.read_pickle('../input/test_target_mean_no_versions.pkl')
#TEST = pd.concat([TEST, test_mean], axis=1)
#del test_mean
print("TEST LOADED")

FOLDS_NR = 5
JOBS_NR = 5
THREAD_NR = 6

folds = StratifiedKFold(n_splits=FOLDS_NR, shuffle=True, random_state=15)
oof = np.zeros(len(TRAIN))
FEATURES = [c for c in TRAIN.columns if c not in ['MachineIdentifier']]

predictions = np.zeros(len(TEST))
feature_importance_df = pd.DataFrame()
score = [0 for _ in range(folds.n_splits)]
work_queue = Queue()
done_queue = Queue()
processes = []



PARAM = {
    'num_leaves': 2048,
    'max_bin': 2000,
    'num_threads': THREAD_NR,
    'objective': 'binary',
    'max_depth': -1,
    'learning_rate': 0.05,
    "boosting": "gbdt",
    "feature_fraction": 0.7,
    "bagging_freq": 1,
    "bagging_fraction": 0.7,
    "bagging_seed": 11,
    "metric": 'auc',
    "lambda_l1": 0.1,
    "random_state": 133,
    "verbosity": -1
}



def worker(work_queue, done_queue, predictions_cache, oof_cache, importance_cache, trn_idx_cache, val_idx_cache):

    try:
        for data in iter(work_queue.get, 'STOP'):
            print("{} GOT DATA".format(current_process().name), data)

            fold = int(data['fold'])

            print("fold", fold, "type", type(fold))

            trn_idx = trn_idx_cache[fold]
            val_idx = val_idx_cache[fold]
            

            
            trn_data = lgb.Dataset(TRAIN.iloc[trn_idx][FEATURES],
                                   label=TARGET.iloc[trn_idx]
                                   )
            val_data = lgb.Dataset(TRAIN.iloc[val_idx][FEATURES],
                                   label=TARGET.iloc[val_idx]
                                   )
            
            clf = lgb.train(PARAM,
                            trn_data,
                            data['num_round'],
                            valid_sets=[trn_data, val_data],
                            verbose_eval=50,
                            early_stopping_rounds=200)

           
            oof_fo = clf.predict(
                    TRAIN.iloc[val_idx][FEATURES], num_iteration=clf.best_iteration)

            oof_cache[fold] = oof_fo
            importance_cache[fold] = clf.feature_importance(importance_type='gain')

            print("{} finished training".format(current_process().name))

           
            initial_idx = 0
            chunk_size = 1000000
            current_pred = np.zeros(len(TEST))
            while initial_idx < TEST.shape[0]:
                final_idx = min(initial_idx + chunk_size, TEST.shape[0])
                idx = range(initial_idx, final_idx)
                current_pred[idx] = clf.predict(
                    TEST.iloc[idx][FEATURES], num_iteration=clf.best_iteration)
                initial_idx = final_idx

            predictions_cache[fold] = current_pred 
            
            result = fold

            print("{} finished prediction".format(current_process().name))
            print("storing result", result)

            done_queue.put(result)
    
    except Exception as e:
        done_queue.put("{} failed with: {}".format(
            current_process().name, e))
    
    print("{} finished".format(current_process().name), time.strftime("%b %d %Y %H:%M:%S"))
    return True

with Manager() as manager:
    
    predictions_cache = manager.dict()
    oof_cache = manager.dict()
    importance_cache = manager.dict()
    trn_idx_cache = manager.dict()
    val_idx_cache = manager.dict()    


    start = time.time()
    print("STARTING PARALLEL K-FOLD CV", time.strftime("%b %d %Y %H:%M:%S"))
    for fold_, (trn_idx, val_idx) in enumerate(folds.split(TRAIN.values, TARGET.values)):
        print("fold nÂ°{}".format(fold_))

        num_round = 10000

        trn_idx_cache[fold_] = trn_idx
        val_idx_cache[fold_] = val_idx

        data = {
            'fold': fold_,
            'num_round': num_round
        }

        work_queue.put(data)
        

    for w in range(JOBS_NR):
        
        p = Process(target=worker, args=(work_queue, done_queue, predictions_cache, oof_cache, importance_cache, trn_idx_cache, val_idx_cache))
        p.start()
        processes.append(p)
        print("started process", w)
        
        work_queue.put('STOP')

        
    for p in processes:
        p.join()
        print('joined', p)



    done_queue.put('STOP')

    print("training completed: ", time.strftime("%b %d %Y %H:%M:%S"))

    print("training time", time.time() - start)

    print("starting final prediction: ", time.strftime("%b %d %Y %H:%M:%S"))
    start = time.time()

    for result in iter(done_queue.get, 'STOP'):
        print("RESULT", result)
        fold_nr = int(result)

    
        val_idx = val_idx_cache[fold_nr]
        

        oof[val_idx] = oof_cache[fold_nr]
        
        fold_importance_df = pd.DataFrame()
        fold_importance_df["feature"] = FEATURES
        fold_importance_df["importance"] = importance_cache[fold_nr]
        fold_importance_df["fold"] = fold_nr + 1
        feature_importance_df = pd.concat(
            [feature_importance_df, fold_importance_df], axis=0)

        predictions += predictions_cache[fold_nr] / FOLDS_NR

        score[fold_nr] = metrics.roc_auc_score(TARGET.iloc[val_idx], oof[val_idx])
      
    print("prediction merge completed: ", time.strftime("%b %d %Y %H:%M:%S"))

    print("prediction time", time.time() - start)


cv_score = metrics.roc_auc_score(TARGET, oof)

cv_score_printable = "{:<8.5f}".format(cv_score)
print("CV score: {}".format(cv_score_printable))

cv_score_printable = cv_score_printable.replace(".", "")
cv_score_printable = cv_score_printable.strip()



cols = (feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:1000].index)

best_FEATURES = feature_importance_df.loc[
    feature_importance_df.feature.isin(cols)]

plt.figure(figsize=(14, 25))
sns.barplot(x="importance",
            y="feature",
            data=best_FEATURES.sort_values(by="importance",
                                           ascending=False))
plt.title('LightGBM Features (avg over folds)')
plt.tight_layout()
plt.savefig(
    'e{}_lgbm_importances_{}.png'.format(experinment_nr, cv_score_printable))
feature_importance_df.to_csv(
    'importances/e{}_lgbm_importances_{}.csv'.format(experinment_nr, cv_score_printable))




sub_df = pd.read_csv('.sample_submission.csv')
sub_df["HasDetections"] = predictions

model_dir = '../output'


model_name = 'submit_e{}_cv{}.csv.gz'.format(
    experinment_nr, cv_score_printable)

fname = os.path.join(model_dir, model_name)
param_string = ', '.join(('{}: {}'.format(k, v) for k, v in PARAM.items()))
message = 'CV: {} DATA: {} LGBM PARAMs: {}'.format(
    cv_score_printable, train_fname, param_string)
competition = 'microsoft-malware-prediction'

sub_df.to_csv(fname, compression='gzip', index=False)
kaggle.api.competition_submit(os.path.abspath(fname), message, competition)

print("script finished: ", time.strftime("%b %d %Y %H:%M:%S"))
